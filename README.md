# The Content in the Repo
- **LWDIFF_in_docker**: The materials for building a docker environment to run LWDIFF.
- **representative_pocs**: The test cases triggering bugs.
- **runtime_info**: The information about the runtimes under test, including the version information and the patch for instrumentation.
- **tester**: The implementation of LWDIFF.
- **tester/GPT_API_ENV**: The code that uses LLM to extract knowledge.


# Tester Usage
Here we introduce how to build a docker image for running LWDIFF.
## Pre-requisites
1. Install docker and mongodb on the host.
2. Use the instructions to build the docker image:
   ```shell
   $ cd LWDIFF_in_docker
   $ docker build -t <image_name> .
   ``` 
3. Build a docker container for LWDIFF
   Basic command:
   ```shell
   $ docker run -it  --network host --name <container_name> <image_name>  /bin/bash
   ```
   In addition, we recommend umounting a folder for storing the testing results.
   ```shell
   $ docker run -it  -v <host_path>:<path_in_container>   --network host   --cpus="<cpu_num>" --cpuset-cpus="<cpu_idxs>" --memory="<memory_limit>"   --name <container_name> <image_name>  /bin/bash
   ```
   Details:
   - We recommend to mount a host path to the container using ` -v <host_path>:<path_in_container>`, and set the output directory of the LWDIFF to the mounted space. 
   - Set the CPU numbers and CPU ids with `--cpus="<cpu_num>" --cpuset-cpus="<cpu_idxs>"`. Since LWDIFF tests the runtimes in parallel, we recommend leaving multiple CPUs in the container. In our experiment, we allocate 10 CPUs for a container.
   - Set the memory limit using `--memory="<memory_limit>"`. In our experiment, we allocate 50G RAM for a container. However, you can also allocate a smaller memory (e.g., 10G, 20G).
   
   For example, a command in our evaluation is 
   ```shell
   $ docker run -it  -v /media/ssd4T1:/media/ssd4T1   -v  /media/ssd_wd1:/media/ssd_wd1  -v  /media/hdd8T1:/media/hdd8T1  --network host   --cpus="10"   --cpuset-cpus="30-39" --memory="50g"   --name tester testing_image:latest  /bin/bash
   ```
4. Run necessary processes on the **host**.
   1. Start the mongod (on the host)
      ```shell
      mongod --config <mongod.conf_path>
      ```
      Note: In our implementation, the MongoDB service is configured to run on port 27016.
      Please, ensure that the mongod instance on the host is properly configured.
   2. Run the process to kill the timeout processes in the background (on the host). 
      ```shell
      $ cd LWDIFF_in_docker
      $ sudo script_remove_dead_process.sh &
      ```
      Specifically, it is designed to terminate runtime processes that have exceeded the specified timeout period. (Note: In our implementation, the timeout for an execution is set to 1 second.) Therefore, please ensure this process remains active during the test. **If you wish to test other runtimes or extend the timeout duration, modify the script accordingly**.

5. Run the tester in the docker
   Enter the folder of the tester in the container
   1. Get the actual runner command
      ```shell
      $ python script_evaluation_runner.py Our <result_sub_dir_name> <result_base_dir> False
      ```
      The command will output the actual command to specify the testing and the path of the result (i.e.,  `<result_directory>`).
   2. Run the command produced by the previous step to conduct the testing. LWDIFF takes a few minutes to parsing the extracted specification, and the testign starts after the parsing.
   3. Run the command to de-duplicate differences using clustering.
      ```shell
      $ python script_analyze_in_result_base_dir.py <result_directory>
      ```
6. Check the testing result
   1. Cov information

      You can visit the code coverage by the command
      ```shell
      $ python see_cov_of_a_key_from_directory.py <result_directory>
      ```
      
      The code coverage can achieve around 30% in 3 hours.
   2. Difference

      The test cases triggering differences is stored in the sub-folder `diff_tcs`. The clustering result is stored in the sub-folder `log_category_base`.


7. Clean the runtime cache in the container

   Run the following command to clear the cache file generated by the runtime
   ```shell
   $ rm -rf ~/.wasmer ~/.cache/wasmtime
   ``` 

8. Run the baselines
   
   We introduce how to run the baselines in [evaluation_description.md](tester/evaluation_description.md)

# LLM Usage
## Pre-requisites
1. Build the docker container environment as described in [Tester Usage](#tester-usage)
2. Update the `api_key` and `base_url` in [api_util.py](./tester/GPT_API_ENV/api_util.py) for OPENAI API service.

## Knowledge Extraction
1. Use the script [script_ask_control_flow.py](./tester/script_ask_control_flow.py) to extract the knowledge about the control flow constructs.
    ```
    python script_ask_control_flow.py
    ```
    
2. Use the script [script_ask_module_def.py](./tester/script_ask_module_def.py) to extract the knowledge about module definitions.
    ```
    python script_ask_module_def.py
    ```
3. Use the script [script_ask_inst.py](./tester/script_ask_inst.py) to extract the knowledge about mhe instructions.
    ```
    python script_ask_inst.py
    ```

## Details of LLM setup
Our implementation is built on GPT-4, accessed through the API.
We set the temperature parameter to zero. We justify the design choice to modulate the randomness and creativity of the LLM to mitigate the LLMâ€™s hallucination problem.

To retrieve the specification knowledge for LLM, we build a knowledge retriever by parsing the Wasm specification with the help of Docutils. 

In our implementation, we combine the four instruction-related tasks into a single prompt, enabling the LLM to complete all tasks together. This approach avoids the inefficiency of using separate prompts, which would require repeatedly providing the same background information and wasting tokens.

We introduce how we use the knowledge at [Knowledge Usage](./other/Knowledge_Usage.pdf).

## Prompt
- Prompt template for querying the format of a module definition: [binary_module_definition_prompt_template.txt](./tester/GPT_API_ENV/prompt_templates/binary_module_definition_prompt_template.txt). [An example](./tester/GPT_API_ENV/prompt_examples/query_memory_definition_binary_format_prompt.txt).

- Prompt template for querying the information about an instruction: [inst_prompt_template.txt](./tester/GPT_API_ENV/prompt_templates/inst_prompt_template.txt). [An example](./tester/GPT_API_ENV/prompt_examples/query_select_prompt.txt).

- Prompt template for querying the grammar describing the control flow constructs: [control_flow_construct_prompt_template.txt](./tester/GPT_API_ENV/prompt_templates/control_flow_construct_prompt_template.txt). [An example](./tester/GPT_API_ENV/prompt_examples/query_control_flow_construct_prompt.txt).
